# Automatic Semantic Annotation of Images (ASAI) code

This is source code for generating automatic semantic of images in the form of sentence based on image pixels and text. Because the system needs images and text input to generate image annotations, the image should come from an article where there is a text that surround the images, such as news web sites.

## Preparing Dependencies
This guide is intended for preparing the dependencies in linux environment.
* Install Anaconda 3 version 4.2.0 (open the installer and follow the instructions)
```shell
wget https://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh
sh Anaconda3-4.2.0-Linux-x86_64.sh
```
* Install opencv3, keras, dask
```shell
conda install -c menpo opencv3
conda install keras
pip install dask --upgrade
```
* Install tensorflow 0.12.1
```shell
wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl
pip3 install tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl
```
* Install NLTK dependencies
```shell
python -c "import nltk;nltk.download('popular')"
```
* The above dependencies should be sufficient to run the code in CPU mode. Using CPU only may require a lot of time during training and GPU is needed to run the code more efficiently. To run the code in the GPU, you need an NVIDIA GPU with CUDA support, CUDA toolkit, and CuDNN. Refer to (https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/tensorflow/) for more details.

## Getting Started

1. **Get the code.** Download this source code.
2. **Get the dataset.** asai needs an image features generated by a cnn. we used cnn from vgg. Download the dataset in [here](http://bit.ly/2CcCjvE) and save them in `./data/` folder.
3. **Train the model.** To train a model of asai, run the asai-train.py code with parameters. `python asai_train.py --num_word_count_threshold=5 --dim_embed=96 --dim_hidden=96 --batch_size=24`
4. **Monitor the training.** during training, execute `tensorboard --logdir=train:./logs/train_logs,validation:./logs/val_logs` and open the url `http://localhost:6006` through a browser. The training may takes several days and you may stop the training (using Ctrl+C in the shell) when validation loss is not decreasing for every iteration.
4. **Evaluate the BLEU performance of a model.** Run the asai_bleu_test.py code and specify the iteration number. `python asai_bleu_test.py --iter=68` We provided a model checkpoint in the `models` folder so you can do evaluation without doing the training first.

## Using your own dataset

The system uses `img-cnn-dataset.npy` and `article-dataset.npy` as the input, which store numpy arrays that contain CNN features from images and articles respectively. `caption-dataset.npy` is used as the ground truth for system evaluation. The CNN features is generated using VGG Net from ILSVRC 2014. You may replace those files with arrays that have the same structure to use your own dataset.
